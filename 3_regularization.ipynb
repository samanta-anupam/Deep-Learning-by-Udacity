{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "output_extras": [
      {
       "item_id": 1.0
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777.0,
     "status": "ok",
     "timestamp": 1.449849322348E12,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480.0
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "output_extras": [
      {
       "item_id": 1.0
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728.0,
     "status": "ok",
     "timestamp": 1.449849322356E12,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480.0
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input Data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_reg = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros(num_labels))\n",
    "\n",
    "    # Training Computation \n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) \\\n",
    "           + beta_reg * tf.nn.l2_loss(weights)\n",
    "\n",
    "    # Optimizer \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        #         print('Initialised')\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % \\\n",
    "                     (train_labels.shape[0] - batch_size)\n",
    "\n",
    "            batch_data = train_dataset[offset:offset + batch_size, :]\n",
    "            batch_labels = train_labels[offset:offset + batch_size, :]\n",
    "\n",
    "            feed_dict = {tf_train_dataset: batch_data, \\\n",
    "                         tf_train_labels: batch_labels, beta_reg: regul}\n",
    "            _, l, predictions = session.run([optimizer, loss, \\\n",
    "                                             train_prediction], feed_dict=feed_dict)\n",
    "        #             if step%500 == 0:\n",
    "        #                 print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        #                 print(\"Minibatch accuracy: %.1f%% \" % accuracy(predictions, batch_labels))\n",
    "        #                 print(\"Validation accuracy: %.1f%% \" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "\n",
    "        accuracy_val.append(accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEMCAYAAADDMN02AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns5trWFQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12c89a3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=\n",
    "    (batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=\n",
    "    (batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Weights\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    beta1 = tf.placeholder(tf.float32)\n",
    "    beta2 = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Training Computation\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) \\\n",
    "           + beta1 * tf.nn.l2_loss(weights1) + beta2 * tf.nn.l2_loss(weights2)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "#         print('Initialised')\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % \\\n",
    "                (train_labels.shape[0] - batch_size)\n",
    "\n",
    "            batch_data = train_dataset[offset:offset+batch_size, :]\n",
    "            batch_labels = train_labels[offset:offset+batch_size, :]\n",
    "\n",
    "            feed_dict = {tf_train_dataset:batch_data, \\\n",
    "                         tf_train_labels:batch_labels, beta1 : regul, beta2 : regul}\n",
    "            _, l, predictions = session.run([optimizer, loss, \\\n",
    "                                             train_prediction], feed_dict=feed_dict)\n",
    "#             if step%500 == 0:\n",
    "#                 print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "#                 print(\"Minibatch accuracy: %.1f%% \" % accuracy(predictions, batch_labels))\n",
    "#                 print(\"Validation accuracy: %.1f%% \" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "\n",
    "        accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEMCAYAAADDMN02AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12cae1668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=\n",
    "                                     (batch_size, image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=\n",
    "                                    (batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Weights\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    beta1 = tf.placeholder(tf.float32)\n",
    "    beta2 = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Training Computation\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\\\n",
    "            + beta1 * tf.nn.l2_loss(weights1) + beta2 * tf.nn.l2_loss(weights2)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised\n",
      "Minibatch loss at step 0: 457.071503\n",
      "Minibatch accuracy: 6.2% \n",
      "Validation accuracy: 28.6% \n",
      "Minibatch loss at step 500: 48.271866\n",
      "Minibatch accuracy: 100.0% \n",
      "Validation accuracy: 57.1% \n",
      "Minibatch loss at step 1000: 43.682327\n",
      "Minibatch accuracy: 100.0% \n",
      "Validation accuracy: 57.1% \n",
      "Minibatch loss at step 1500: 39.801270\n",
      "Minibatch accuracy: 100.0% \n",
      "Validation accuracy: 57.1% \n",
      "Minibatch loss at step 2000: 36.433441\n",
      "Minibatch accuracy: 100.0% \n",
      "Validation accuracy: 57.1% \n",
      "Minibatch loss at step 2500: 33.454212\n",
      "Minibatch accuracy: 100.0% \n",
      "Validation accuracy: 57.2% \n",
      "Minibatch loss at step 3000: 30.782091\n",
      "Minibatch accuracy: 100.0% \n",
      "Validation accuracy: 57.2% \n",
      "Test accuracy: 63.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "num_batches = 3\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialised')\n",
    "    for step in range(num_steps):\n",
    "        offset = step%num_batches\n",
    "\n",
    "        batch_data = train_dataset[offset:offset+batch_size, :]\n",
    "        batch_labels = train_labels[offset:offset+batch_size, :]\n",
    "\n",
    "        feed_dict = {tf_train_dataset:batch_data, \\\n",
    "                     tf_train_labels:batch_labels, beta1 : regul, beta2 : 1e-3}\n",
    "        _, l, predictions = session.run([optimizer, loss, \\\n",
    "                                         train_prediction], feed_dict=feed_dict)\n",
    "        if step%500 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%% \" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%% \" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "#     accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=\n",
    "    (batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=\n",
    "    (batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Weights\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    beta1 = tf.placeholder(tf.float32)\n",
    "    beta2 = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Training Computation\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "    logits = tf.matmul(drop1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) \\\n",
    "           + beta1 * tf.nn.l2_loss(weights1) + beta2 * tf.nn.l2_loss(weights2)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised\n",
      "Minibatch loss at step 0: 565.550659\n",
      "Minibatch accuracy: 7.8% \n",
      "Validation accuracy: 26.8% \n",
      "Minibatch loss at step 500: 82.385880\n",
      "Minibatch accuracy: 75.0% \n",
      "Validation accuracy: 80.3% \n",
      "Minibatch loss at step 1000: 63.704960\n",
      "Minibatch accuracy: 71.1% \n",
      "Validation accuracy: 79.8% \n",
      "Minibatch loss at step 1500: 47.734917\n",
      "Minibatch accuracy: 82.8% \n",
      "Validation accuracy: 79.9% \n",
      "Minibatch loss at step 2000: 39.537540\n",
      "Minibatch accuracy: 81.2% \n",
      "Validation accuracy: 80.5% \n",
      "Minibatch loss at step 2500: 35.459290\n",
      "Minibatch accuracy: 73.4% \n",
      "Validation accuracy: 80.5% \n",
      "Minibatch loss at step 3000: 30.795641\n",
      "Minibatch accuracy: 78.9% \n",
      "Validation accuracy: 80.5% \n",
      "Test accuracy: 88.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialised')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % \\\n",
    "                (train_labels.shape[0] - batch_size)\n",
    "\n",
    "        batch_data = train_dataset[offset:offset+batch_size, :]\n",
    "        batch_labels = train_labels[offset:offset+batch_size, :]\n",
    "\n",
    "        feed_dict = {tf_train_dataset:batch_data, \\\n",
    "                     tf_train_labels:batch_labels, beta1 : regul, beta2 : regul}\n",
    "        _, l, predictions = session.run([optimizer, loss, \\\n",
    "                                         train_prediction], feed_dict=feed_dict)\n",
    "        if step%500 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%% \" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%% \" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1024)\n",
      "(128, 256)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = [1024, 256, 128]\n",
    "input_size = [image_size * image_size] + num_hidden_nodes\n",
    "num_layers = len(num_hidden_nodes)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=\n",
    "    (batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=\n",
    "    (batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "\n",
    "    # Weights\n",
    "    weights1 = []\n",
    "    biases1 = []\n",
    "    beta1 = []\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        weights1.append(tf.Variable(tf.truncated_normal([input_size[i], num_hidden_nodes[i]], \\\n",
    "                                                        stddev=np.sqrt(2.0 / (input_size[i])))))\n",
    "        biases1.append(tf.Variable(tf.zeros([num_hidden_nodes[i]])))\n",
    "        beta1.append(1e-3)\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([input_size[-1], num_labels], \\\n",
    "                                               stddev=np.sqrt(2.0 / input_size[-1])))\n",
    "    beta2 = 1e-3\n",
    "\n",
    "    # Training Computation\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1[0]) + biases1[0])\n",
    "    drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "    for i in range(1, num_layers):\n",
    "        print(lay1_train.shape)\n",
    "        lay1_train = tf.nn.relu(tf.matmul(drop1, weights1[i]) + biases1[i])\n",
    "        drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "    logits = tf.matmul(drop1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) \\\n",
    "           + beta2 * tf.nn.l2_loss(weights2)\n",
    "    for i in range(num_layers):\n",
    "        loss += beta1[i] * tf.nn.l2_loss(weights1[i])\n",
    "\n",
    "        # Optimizer\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1[0]) + biases1[0])\n",
    "    for i in range(1, num_layers):\n",
    "        lay1_valid = tf.nn.relu(tf.matmul(lay1_valid, weights1[i]) + biases1[i])\n",
    "    logits_valid = tf.matmul(lay1_valid, weights2) + biases2\n",
    "    valid_prediction = tf.nn.softmax(logits_valid)\n",
    "\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1[0]) + biases1[0])\n",
    "    for i in range(1, num_layers):\n",
    "        lay1_test = tf.nn.relu(tf.matmul(lay1_test, weights1[i]) + biases1[i])\n",
    "    logits_test = tf.matmul(lay1_test, weights2) + biases2\n",
    "    test_prediction = tf.nn.softmax(logits_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised\n",
      "Minibatch loss at step 0: 3.871848\n",
      "Minibatch accuracy: 10.9% \n",
      "Validation accuracy: 18.4% \n",
      "Minibatch loss at step 500: 1.205388\n",
      "Minibatch accuracy: 85.9% \n",
      "Validation accuracy: 83.8% \n",
      "Minibatch loss at step 1000: 1.051275\n",
      "Minibatch accuracy: 85.2% \n",
      "Validation accuracy: 84.2% \n",
      "Minibatch loss at step 1500: 0.844605\n",
      "Minibatch accuracy: 85.2% \n",
      "Validation accuracy: 85.5% \n",
      "Minibatch loss at step 2000: 0.706294\n",
      "Minibatch accuracy: 88.3% \n",
      "Validation accuracy: 86.3% \n",
      "Minibatch loss at step 2500: 0.737942\n",
      "Minibatch accuracy: 85.9% \n",
      "Validation accuracy: 86.9% \n",
      "Minibatch loss at step 3000: 0.748825\n",
      "Minibatch accuracy: 85.2% \n",
      "Validation accuracy: 87.0% \n",
      "Minibatch loss at step 3500: 0.767259\n",
      "Minibatch accuracy: 85.2% \n",
      "Validation accuracy: 87.7% \n",
      "Minibatch loss at step 4000: 0.636193\n",
      "Minibatch accuracy: 89.8% \n",
      "Validation accuracy: 87.6% \n",
      "Minibatch loss at step 4500: 0.660385\n",
      "Minibatch accuracy: 89.1% \n",
      "Validation accuracy: 87.9% \n",
      "Minibatch loss at step 5000: 0.609464\n",
      "Minibatch accuracy: 88.3% \n",
      "Validation accuracy: 88.2% \n",
      "Minibatch loss at step 5500: 0.682371\n",
      "Minibatch accuracy: 87.5% \n",
      "Validation accuracy: 88.3% \n",
      "Minibatch loss at step 6000: 0.636231\n",
      "Minibatch accuracy: 84.4% \n",
      "Validation accuracy: 88.7% \n",
      "Minibatch loss at step 6500: 0.478164\n",
      "Minibatch accuracy: 89.8% \n",
      "Validation accuracy: 88.6% \n",
      "Minibatch loss at step 7000: 0.622200\n",
      "Minibatch accuracy: 85.2% \n",
      "Validation accuracy: 88.9% \n",
      "Minibatch loss at step 7500: 0.655744\n",
      "Minibatch accuracy: 84.4% \n",
      "Validation accuracy: 88.9% \n",
      "Minibatch loss at step 8000: 0.779443\n",
      "Minibatch accuracy: 80.5% \n",
      "Validation accuracy: 89.0% \n",
      "Minibatch loss at step 8500: 0.506388\n",
      "Minibatch accuracy: 89.8% \n",
      "Validation accuracy: 89.0% \n",
      "Minibatch loss at step 9000: 0.589692\n",
      "Minibatch accuracy: 88.3% \n",
      "Validation accuracy: 89.0% \n",
      "Minibatch loss at step 9500: 0.610356\n",
      "Minibatch accuracy: 86.7% \n",
      "Validation accuracy: 89.0% \n",
      "Minibatch loss at step 10000: 0.561710\n",
      "Minibatch accuracy: 86.7% \n",
      "Validation accuracy: 89.0% \n",
      "Minibatch loss at step 10500: 0.535919\n",
      "Minibatch accuracy: 89.1% \n",
      "Validation accuracy: 89.1% \n",
      "Minibatch loss at step 11000: 0.512945\n",
      "Minibatch accuracy: 91.4% \n",
      "Validation accuracy: 89.1% \n",
      "Minibatch loss at step 11500: 0.591603\n",
      "Minibatch accuracy: 87.5% \n",
      "Validation accuracy: 89.1% \n",
      "Minibatch loss at step 12000: 0.604521\n",
      "Minibatch accuracy: 85.2% \n",
      "Validation accuracy: 89.1% \n",
      "Minibatch loss at step 12500: 0.496303\n",
      "Minibatch accuracy: 89.8% \n",
      "Validation accuracy: 89.2% \n",
      "Minibatch loss at step 13000: 0.592863\n",
      "Minibatch accuracy: 86.7% \n",
      "Validation accuracy: 89.2% \n",
      "Minibatch loss at step 13500: 0.610308\n",
      "Minibatch accuracy: 85.9% \n",
      "Validation accuracy: 89.2% \n",
      "Minibatch loss at step 14000: 0.580676\n",
      "Minibatch accuracy: 85.9% \n",
      "Validation accuracy: 89.2% \n",
      "Minibatch loss at step 14500: 0.643327\n",
      "Minibatch accuracy: 85.2% \n",
      "Validation accuracy: 89.2% \n",
      "Minibatch loss at step 15000: 0.582042\n",
      "Minibatch accuracy: 88.3% \n",
      "Validation accuracy: 89.2% \n",
      "Minibatch loss at step 15500: 0.630062\n",
      "Minibatch accuracy: 84.4% \n",
      "Validation accuracy: 89.2% \n",
      "Minibatch loss at step 16000: 0.442700\n",
      "Minibatch accuracy: 91.4% \n",
      "Validation accuracy: 89.3% \n",
      "Minibatch loss at step 16500: 0.527453\n",
      "Minibatch accuracy: 89.8% \n",
      "Validation accuracy: 89.2% \n",
      "Minibatch loss at step 17000: 0.473067\n",
      "Minibatch accuracy: 93.0% \n",
      "Validation accuracy: 89.2% \n",
      "Minibatch loss at step 17500: 0.338154\n",
      "Minibatch accuracy: 94.5% \n",
      "Validation accuracy: 89.2% \n",
      "Minibatch loss at step 18000: 0.439503\n",
      "Minibatch accuracy: 93.8% \n",
      "Validation accuracy: 89.2% \n",
      "Test accuracy: 95.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "accuracy_val = []\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialised')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % \\\n",
    "                 (train_labels.shape[0] - batch_size)\n",
    "\n",
    "        batch_data = train_dataset[offset:offset + batch_size, :]\n",
    "        batch_labels = train_labels[offset:offset + batch_size, :]\n",
    "\n",
    "        feed_dict = {tf_train_dataset: batch_data, \\\n",
    "                     tf_train_labels: batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, \\\n",
    "                                         train_prediction], feed_dict=feed_dict)\n",
    "        if step % 500 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%% \" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%% \" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
